{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example is very similar to c17e4_nas_random_hill, but it uses an evolutionary search algorithm instead of random search and hill climbing. More context for this code example can be found in the section \"Programming Example: Searching for an architecture for CIFAR-10 classification\" in Chapter 17 in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358).\n",
    "\n",
    "The initial part of the programming example is identical to c17e4_nas_random_hill with the addition of a constant POPULATION_SIZE which is used by the evolutionary algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from utilities import train_model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_MODEL_SIZE = 500000\n",
    "CANDIDATE_EVALUATIONS = 500\n",
    "EVAL_EPOCHS = 3\n",
    "FINAL_EPOCHS = 20\n",
    "POPULATION_SIZE = 50\n",
    "\n",
    "layer_types = ['DENSE', 'CONV2D', 'MAXPOOL2D']\n",
    "param_values = dict([('size', [16, 64, 256, 1024, 4096]),\n",
    "                ('activation', ['relu', 'tanh', 'elu']),\n",
    "                ('kernel_size', [(1, 1), (2, 2), (3, 3), (4, 4)]),\n",
    "                ('stride', [(1, 1), (2, 2), (3, 3), (4, 4)]),\n",
    "                ('dropout', [0.0, 0.4, 0.7, 0.9])])\n",
    "\n",
    "layer_params = dict([('DENSE', ['size', 'activation', 'dropout']),\n",
    "                     ('CONV2D', ['size', 'activation',\n",
    "                                 'kernel_size', 'stride',\n",
    "                                 'dropout']),\n",
    "                     ('MAXPOOL2D', ['kernel_size', 'stride',\n",
    "                                    'dropout'])])\n",
    "\n",
    "# Load training dataset into a single batch to compute mean and stddev.\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = CIFAR10(root='./pt_data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "data = next(iter(trainloader))\n",
    "mean = data[0].mean()\n",
    "stddev = data[0].std()\n",
    "\n",
    "# Load and standardize training and test dataset.\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean, stddev)])\n",
    "\n",
    "trainset = CIFAR10(root='./pt_data', train=True, download=True, transform=transform)\n",
    "testset = CIFAR10(root='./pt_data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Methods to create a model definition.\n",
    "def generate_random_layer(layer_type):\n",
    "    layer = {}\n",
    "    layer['layer_type'] = layer_type\n",
    "    params = layer_params[layer_type]\n",
    "    for param in params:\n",
    "        values = param_values[param]\n",
    "        layer[param] = values[np.random.randint(0, len(values))]\n",
    "    return layer\n",
    "\n",
    "def generate_model_definition():\n",
    "    layer_count = np.random.randint(2, 9)\n",
    "    non_dense_count = np.random.randint(1, layer_count)\n",
    "    layers = []\n",
    "    for i in range(layer_count):\n",
    "        if i < non_dense_count:\n",
    "            layer_type = layer_types[np.random.randint(1, 3)]\n",
    "            layer = generate_random_layer(layer_type)\n",
    "        else:\n",
    "            layer = generate_random_layer('DENSE')\n",
    "        layers.append(layer)\n",
    "    return layers\n",
    "\n",
    "def compute_weight_count(layers):\n",
    "    last_shape = (32, 32, 3)\n",
    "    total_weights = 0\n",
    "    for layer in layers:\n",
    "        layer_type = layer['layer_type']\n",
    "        if layer_type == 'DENSE':\n",
    "            size = layer['size']\n",
    "            weights = size * (np.prod(last_shape) + 1)\n",
    "            last_shape = (layer['size'])\n",
    "        else:\n",
    "            stride = layer['stride']\n",
    "            if layer_type == 'CONV2D':\n",
    "                size = layer['size']\n",
    "                kernel_size = layer['kernel_size']\n",
    "                weights = size * ((np.prod(kernel_size) *\n",
    "                                   last_shape[2]) + 1)\n",
    "                last_shape = (np.ceil(last_shape[0]/stride[0]),\n",
    "                              np.ceil(last_shape[1]/stride[1]),\n",
    "                              size)\n",
    "            elif layer_type == 'MAXPOOL2D':\n",
    "                weights = 0\n",
    "                last_shape = (np.ceil(last_shape[0]/stride[0]),\n",
    "                              np.ceil(last_shape[1]/stride[1]),\n",
    "                              last_shape[2])\n",
    "        total_weights += weights\n",
    "    total_weights += ((np.prod(last_shape) + 1) * 10)\n",
    "    return total_weights\n",
    "\n",
    "# Methods to create and evaluate model based on model definition.\n",
    "def add_layer(model_list, params, prior_type, last_shape):\n",
    "    layer_num = len(model_list)\n",
    "    act = None\n",
    "    layer_type = params['layer_type']\n",
    "    if layer_type == 'DENSE':\n",
    "        if prior_type != 'DENSE':\n",
    "            model_list.append((\"layer\" + str(layer_num), nn.Flatten()))\n",
    "            layer_num += 1\n",
    "            last_shape = int(np.prod(last_shape))\n",
    "        size = params['size']\n",
    "        act = params['activation']\n",
    "        model_list.append((\"layer\" + str(layer_num), nn.Linear(last_shape, size)))\n",
    "        last_shape = (size)\n",
    "    elif layer_type == 'CONV2D':\n",
    "        size = params['size']\n",
    "        act = params['activation']\n",
    "        kernel_size = params['kernel_size']\n",
    "        stride = params['stride']\n",
    "        padding = int(kernel_size[0] / 2)\n",
    "        model_list.append((\"layer\" + str(layer_num),\n",
    "                           nn.Conv2d(last_shape[2], size, kernel_size, stride=stride, padding=padding)))\n",
    "        last_shape = (int((last_shape[0]+2*padding-(kernel_size[0]-1)-1)/stride[0]+1),\n",
    "                      int((last_shape[1]+2*padding-(kernel_size[1]-1)-1)/stride[1]+1),\n",
    "                      size)\n",
    "    elif layer_type == 'MAXPOOL2D':\n",
    "        kernel_size = params['kernel_size']\n",
    "        stride = params['stride']\n",
    "        padding = int(kernel_size[0] / 2)\n",
    "        model_list.append((\"layer\" + str(layer_num),\n",
    "                           nn.MaxPool2d(kernel_size, stride=stride, padding=padding)))\n",
    "        last_shape = (int((last_shape[0]+2*padding-(kernel_size[0]-1)-1)/stride[0]+1),\n",
    "                      int((last_shape[1]+2*padding-(kernel_size[1]-1)-1)/stride[1]+1),\n",
    "                      last_shape[2])\n",
    "    layer_num += 1\n",
    "    if(act != None):\n",
    "        if (act == 'relu'):\n",
    "            model_list.append((\"layer\" + str(layer_num), nn.ReLU()))\n",
    "        elif (act == 'elu'):\n",
    "            model_list.append((\"layer\" + str(layer_num), nn.ELU()))\n",
    "        elif (act == 'tanh'):\n",
    "            model_list.append((\"layer\" + str(layer_num), nn.Tanh()))\n",
    "        layer_num += 1\n",
    "    dropout = params['dropout']\n",
    "    if(dropout > 0.0):\n",
    "        model_list.append((\"layer\" + str(layer_num), nn.Dropout(p=dropout)))\n",
    "    return last_shape\n",
    "\n",
    "def create_model(layers):\n",
    "    model_list = []\n",
    "    prev_layer = 'NONE'\n",
    "    last_shape = (32, 32, 3)\n",
    "    for layer in layers:\n",
    "        last_shape = add_layer(model_list, layer, prev_layer, last_shape)\n",
    "        prev_layer = layer['layer_type']\n",
    "    model_list.append((\"layer\" + str(len(model_list)), nn.Linear(last_shape, 10)))\n",
    "    model = nn.Sequential(OrderedDict(model_list))\n",
    "    return model\n",
    "\n",
    "def create_and_evaluate_model(model_definition):\n",
    "    weight_count = compute_weight_count(model_definition)\n",
    "    if weight_count > MAX_MODEL_SIZE:\n",
    "        return 0.0\n",
    "    model = create_model(model_definition)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    train_result = train_model(model, device, EVAL_EPOCHS, 64, trainset, testset,\n",
    "                               optimizer, loss_function, 'acc')\n",
    "    acc = train_result[1]\n",
    "    print('Size: ', weight_count)\n",
    "    print('Accuracy: %5.2f' %acc)\n",
    "    del model\n",
    "    return acc\n",
    "\n",
    "# Helper method for evolutionary algorithm.\n",
    "def tweak_model(model_definition):\n",
    "    layer_num = np.random.randint(0, len(model_definition))\n",
    "    last_layer = len(model_definition) - 1\n",
    "    for first_dense, layer in enumerate(model_definition):\n",
    "        if layer['layer_type'] == 'DENSE':\n",
    "            break\n",
    "    if np.random.randint(0, 2) == 1:\n",
    "        delta = 1\n",
    "    else:\n",
    "        delta = -1\n",
    "    if np.random.randint(0, 2) == 1:\n",
    "        # Add/remove layer.\n",
    "        if len(model_definition) < 3:\n",
    "            delta = 1 # Layer removal not allowed\n",
    "        if delta == -1:\n",
    "            # Remove layer.\n",
    "            if layer_num == 0 and first_dense == 1:\n",
    "                layer_num += 1 # Require >= 1 non-dense layer\n",
    "            if layer_num == first_dense and layer_num == last_layer:\n",
    "                layer_num -= 1 # Require >= 1 dense layer\n",
    "            del model_definition[layer_num]\n",
    "        else:\n",
    "            # Add layer.\n",
    "            if layer_num < first_dense:\n",
    "                layer_type = layer_types[np.random.randint(1, 3)]\n",
    "            else:\n",
    "                layer_type = 'DENSE'\n",
    "            layer = generate_random_layer(layer_type)\n",
    "            model_definition.insert(layer_num, layer)\n",
    "    else:\n",
    "        # Tweak parameter.\n",
    "        layer = model_definition[layer_num]\n",
    "        layer_type = layer['layer_type']\n",
    "        params = layer_params[layer_type]\n",
    "        param = params[np.random.randint(0, len(params))]\n",
    "        current_val = layer[param]\n",
    "        values = param_values[param]\n",
    "        index = values.index(current_val)\n",
    "        max_index = len(values)\n",
    "        new_val = values[(index + delta) % max_index]\n",
    "        layer[param] = new_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key part of the evolutionary algorithm is the crossover operation, which combines two existing solutions (parents) into a new solution (child) that inherits properties of both of its parents. It is implemented in the code snippet below (see book for more details).\n",
    "\n",
    "The evolutionary algorithm starts by generating and evaluating a population of random models. It then randomly generates new models by tweaking and combining models in the existing population. There are three ways that a new model can be created:\n",
    "- Tweak an existing model.\n",
    "- Combine two parent models into a child model.\n",
    "- Combine two parent models into a child model and apply a tweak to the resulting model.\n",
    "\n",
    "Once new models have been generated, the algorithm probabilistically selects high-performing models to keep for the next iteration. In this selection process, both the parents and the children participate, which is also known as elitism within the field of evolutionary computation.\n",
    "\n",
    "The code first generates and evaluates a population of 50 random models. It then repeatedly evolves and evaluates a new population of 50 individuals. The number of generations is implicitly defined by the two variables CANDIDATE_EVALUATIONS and POPULATION_SIZE. With the default values you will see the algorithm run for ten generations, or 500 individuals in total.\n",
    "\n",
    "Note that although this algorithm is more complex than the hill climbing algorithm in c17e4_nas_random_hill, it does not necessarily result in a better model. The main purpose of these programming examples was not to arrive at the most optimized solution but to illustrate and demystify different approaches to automatically finding a network architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for evolutionary algorithm.\n",
    "def cross_over(parents):\n",
    "    # Pick bottom half of one and top half of the other.\n",
    "    # If model is small, randomly stack top or bottom from both.\n",
    "    bottoms = [[], []]\n",
    "    tops = [[], []]\n",
    "    for i, model in enumerate(parents):\n",
    "        for layer in model:\n",
    "            if layer['layer_type'] != 'DENSE':\n",
    "                bottoms[i].append(copy.deepcopy(layer))\n",
    "            else:\n",
    "                tops[i].append(copy.deepcopy(layer))\n",
    "\n",
    "    i = np.random.randint(0, 2)\n",
    "    if (i == 1 and compute_weight_count(parents[0]) +\n",
    "        compute_weight_count(parents[1]) < MAX_MODEL_SIZE):\n",
    "        i = np.random.randint(0, 2)\n",
    "        new_model = bottoms[i] + bottoms[(i+1)%2]\n",
    "        i = np.random.randint(0, 2)\n",
    "        new_model = new_model + tops[i] + tops[(i+1)%2]\n",
    "    else:\n",
    "        i = np.random.randint(0, 2)\n",
    "        new_model = bottoms[i] + tops[(i+1)%2]\n",
    "    return new_model\n",
    "\n",
    "# Evolutionary algorithm.\n",
    "np.random.seed(7)\n",
    "\n",
    "# Generate initial population of models.\n",
    "population = []\n",
    "for i in range(POPULATION_SIZE):\n",
    "    valid_model = False\n",
    "    while(valid_model == False):\n",
    "        model_definition = generate_model_definition()\n",
    "        acc = create_and_evaluate_model(model_definition)\n",
    "        if acc > 0.0:\n",
    "            valid_model = True\n",
    "    population.append((acc, model_definition))\n",
    "\n",
    "# Evolve population.\n",
    "generations = int(CANDIDATE_EVALUATIONS / POPULATION_SIZE) - 1\n",
    "for i in range(generations):\n",
    "    # Generate new individuals.\n",
    "    print('Generation number: ', i)\n",
    "    for j in range(POPULATION_SIZE):\n",
    "        valid_model = False\n",
    "        while(valid_model == False):\n",
    "            rand = np.random.rand()\n",
    "            parents = random.sample(\n",
    "                population[:POPULATION_SIZE], 2)\n",
    "            parents = [parents[0][1], parents[1][1]]\n",
    "            if rand < 0.5:\n",
    "                child = copy.deepcopy(parents[0])\n",
    "                tweak_model(child)\n",
    "            elif rand < 0.75:\n",
    "                child = cross_over(parents)\n",
    "            else:\n",
    "                child = cross_over(parents)\n",
    "                tweak_model(child)\n",
    "            acc = create_and_evaluate_model(child)\n",
    "            if acc > 0.0:\n",
    "                valid_model = True\n",
    "        population.append((acc, child))\n",
    "    # Randomly select fit individuals.\n",
    "    population.sort(key=lambda x:x[0])\n",
    "    print('Evolution, best accuracy: %5.2f' %population[-1][0])\n",
    "    top = np.int64(np.ceil(0.2*len(population)))\n",
    "    bottom = np.int64(np.ceil(0.3*len(population)))\n",
    "    top_individuals = population[-top:]\n",
    "    remaining = np.int64(len(population)/2) - len(top_individuals)\n",
    "    population = random.sample(population[bottom:-top],\n",
    "                               remaining) + top_individuals\n",
    "\n",
    "best_model = population[-1][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for c17e4_nas_random_hill, we conclude with evaluating the best model for a larger number of epochs. The results can vary significantly from run to run given that all three search algorithms are stochastic. Our results indicate that the hill climbing algorithm is better than the specific evolutionary algorithm we implemented, and both of them are better than the pure random search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final model for larger number of epochs.\n",
    "model = create_model(best_model)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "train_result = train_model(model, device, FINAL_EPOCHS, 64, trainset, testset,\n",
    "                  optimizer, loss_function, 'acc')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
