{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example demonstrates how to use Neural Architecture Search (NAS) to find a suitable architecture for CIFAR-10 classification. We implement random search and hill climbing. More context for this code example can be found in the section \"Programming Example: Searching for an architecture for CIFAR-10 classification\" in Chapter 17 in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358).\n",
    "\n",
    "We start with initialization code and loading the dataset in the code snippet below. We define some variables that are part of defining the search space, such as what types of layers can be used and what kind of parameters and values are valid for each type of layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "import numpy as np\n",
    "import logging\n",
    "import copy\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "MAX_MODEL_SIZE = 500000\n",
    "CANDIDATE_EVALUATIONS = 500\n",
    "EVAL_EPOCHS = 3\n",
    "FINAL_EPOCHS = 20\n",
    "\n",
    "layer_types = ['DENSE', 'CONV2D', 'MAXPOOL2D']\n",
    "param_values = dict([('size', [16, 64, 256, 1024, 4096]),\n",
    "                ('activation', ['relu', 'tanh', 'elu']),\n",
    "                ('kernel_size', [(1, 1), (2, 2), (3, 3), (4, 4)]),\n",
    "                ('stride', [(1, 1), (2, 2), (3, 3), (4, 4)]),\n",
    "                ('dropout', [0.0, 0.4, 0.7, 0.9])])\n",
    "\n",
    "layer_params = dict([('DENSE', ['size', 'activation', 'dropout']),\n",
    "                     ('CONV2D', ['size', 'activation',\n",
    "                                 'kernel_size', 'stride',\n",
    "                                 'dropout']),\n",
    "                     ('MAXPOOL2D', ['kernel_size', 'stride',\n",
    "                                    'dropout'])])\n",
    "\n",
    "# Load dataset.\n",
    "cifar_dataset = keras.datasets.cifar10\n",
    "(train_images, train_labels), (test_images,\n",
    "                    test_labels) = cifar_dataset.load_data()\n",
    "\n",
    "# Standardize dataset.\n",
    "mean = np.mean(train_images)\n",
    "stddev = np.std(train_images)\n",
    "train_images = (train_images - mean) / stddev\n",
    "test_images = (test_images - mean) / stddev\n",
    "\n",
    "# Change labels to one-hot.\n",
    "train_labels = to_categorical(train_labels,\n",
    "                              num_classes=10)\n",
    "test_labels = to_categorical(test_labels,\n",
    "                             num_classes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build some infrastructure for automatically generating models. To keep things simple, we impose significant restrictions on the search space. To start with, we allow only sequential models. In addition, given our knowledge of the application (image classification), we impose a rigid structure on the network. We view the network as a combination of a bottom subnetwork and a top subnetwork. The bottom part consists of a combination of convolutional and maxpooling layers, and the top part consists of fully connected layers. In addition, we allow dropout layers after any layer, and we also add a flatten layer between the bottom and the top to ensure that we end up with a valid TensorFlow model.\n",
    "\n",
    "The methods in the code snippet below are used to generate a random model within this constrained search space. There is also a method that computes the size of the resulting model in terms of the number of trainable parameters. Note that these methods do not have anything to do with TensorFlow but is our own representation of a network before invoking the DL framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to create a model definition.\n",
    "def generate_random_layer(layer_type):\n",
    "    layer = {}\n",
    "    layer['layer_type'] = layer_type\n",
    "    params = layer_params[layer_type]\n",
    "    for param in params:\n",
    "        values = param_values[param]\n",
    "        layer[param] = values[np.random.randint(0, len(values))]\n",
    "    return layer\n",
    "\n",
    "def generate_model_definition():\n",
    "    layer_count = np.random.randint(2, 9)\n",
    "    non_dense_count = np.random.randint(1, layer_count)\n",
    "    layers = []\n",
    "    for i in range(layer_count):\n",
    "        if i < non_dense_count:\n",
    "            layer_type = layer_types[np.random.randint(1, 3)]\n",
    "            layer = generate_random_layer(layer_type)\n",
    "        else:\n",
    "            layer = generate_random_layer('DENSE')\n",
    "        layers.append(layer)\n",
    "    return layers\n",
    "\n",
    "def compute_weight_count(layers):\n",
    "    last_shape = (32, 32, 3)\n",
    "    total_weights = 0\n",
    "    for layer in layers:\n",
    "        layer_type = layer['layer_type']\n",
    "        if layer_type == 'DENSE':\n",
    "            size = layer['size']\n",
    "            weights = size * (np.prod(last_shape) + 1)\n",
    "            last_shape = (layer['size'])\n",
    "        else:\n",
    "            stride = layer['stride']\n",
    "            if layer_type == 'CONV2D':\n",
    "                size = layer['size']\n",
    "                kernel_size = layer['kernel_size']\n",
    "                weights = size * ((np.prod(kernel_size) *\n",
    "                                   last_shape[2]) + 1)\n",
    "                last_shape = (np.ceil(last_shape[0]/stride[0]),\n",
    "                              np.ceil(last_shape[1]/stride[1]),\n",
    "                              size)\n",
    "            elif layer_type == 'MAXPOOL2D':\n",
    "                weights = 0\n",
    "                last_shape = (np.ceil(last_shape[0]/stride[0]),\n",
    "                              np.ceil(last_shape[1]/stride[1]),\n",
    "                              last_shape[2])\n",
    "        total_weights += weights\n",
    "    total_weights += ((np.prod(last_shape) + 1) * 10)\n",
    "    return total_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of methods takes the model definition created in the previous code snippet and creates and evaluates a corresponding TensorFlow model for a small number of epochs. This is all shown in the next code snippet. The method that evaluates the model imposes a size restriction. If the requested model has too many parameters, the method simply returns an accuracy of 0.0. The search algorithm that invokes the method will need to check for this and, if needed, generate a smaller model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to create and evaluate model based on model definition.\n",
    "def add_layer(model, params, prior_type):\n",
    "    layer_type = params['layer_type']\n",
    "    if layer_type == 'DENSE':\n",
    "        if prior_type != 'DENSE':\n",
    "            model.add(Flatten())\n",
    "        size = params['size']\n",
    "        act = params['activation']\n",
    "        model.add(Dense(size, activation=act))\n",
    "    elif layer_type == 'CONV2D':\n",
    "        size = params['size']\n",
    "        act = params['activation']\n",
    "        kernel_size = params['kernel_size']\n",
    "        stride = params['stride']\n",
    "        model.add(Conv2D(size, kernel_size, activation=act,\n",
    "                         strides=stride, padding='same'))\n",
    "    elif layer_type == 'MAXPOOL2D':\n",
    "        kernel_size = params['kernel_size']\n",
    "        stride = params['stride']\n",
    "        model.add(MaxPooling2D(pool_size=kernel_size,\n",
    "                               strides=stride, padding='same'))\n",
    "    dropout = params['dropout']\n",
    "    if(dropout > 0.0):\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "def create_model(layers):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x, input_shape=(32, 32, 3)))\n",
    "    prev_layer = 'LAMBDA' # Dummy layer to set input_shape\n",
    "    for layer in layers:\n",
    "        add_layer(model, layer, prev_layer)\n",
    "        prev_layer = layer['layer_type']\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_and_evaluate_model(model_definition):\n",
    "    weight_count = compute_weight_count(model_definition)\n",
    "    if weight_count > MAX_MODEL_SIZE:\n",
    "        return 0.0\n",
    "    model = create_model(model_definition)\n",
    "    history = model.fit(train_images, train_labels,\n",
    "                        validation_data=(test_images, test_labels),\n",
    "                        epochs=EVAL_EPOCHS, batch_size=64,\n",
    "                        verbose=2, shuffle=False)\n",
    "    acc = history.history['val_accuracy'][-1]\n",
    "    print('Size: ', weight_count)\n",
    "    print('Accuracy: %5.2f' %acc)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the building blocks to implement pure random search. This is shown in the code snippet below. It consists of an outer for loop that runs for a fixed number of iterations. Each iteration randomly generates and evaluates a model. There is an inner loop to handle the case when the generated model is too big. The inner loop simply repeatedly generates random models until one is generated that adheres to the size restriction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure random search.\n",
    "np.random.seed(7)\n",
    "val_accuracy = 0.0\n",
    "for i in range(CANDIDATE_EVALUATIONS):\n",
    "    valid_model = False\n",
    "    while(valid_model == False):\n",
    "        model_definition = generate_model_definition()\n",
    "        acc = create_and_evaluate_model(model_definition)\n",
    "        if acc > 0.0:\n",
    "            valid_model = True\n",
    "    if acc > val_accuracy:\n",
    "        best_model = model_definition\n",
    "        val_accuracy = acc\n",
    "    print('Random search, best accuracy: %5.2f' %val_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement the hill climbing algorithm (see book for details). This is done in the next code snippet. We create a helper method that randomly adjusts one of the parameters slightly to move an existing model into a neighboring model in the allowed search space. The first for loop determines the index of the boundary between the bottom (non-dense) and top (dense) layers. The next step is to determine whether to increase or decrease the capacity of the model. This is followed by determining whether to add/remove a layer or tweak parameters of an existing layer. Much of the logic is there to ensure that the modified model still stays within the boundaries of what is a legal model.\n",
    "\n",
    "The actual hill climbing algorithm is implemented at the bottom of the code snippet. It assumes an initial model and gradually tweaks it in the direction that improves prediction accuracy. The implemented version of the algorithm is known as stochastic hill climbing. A parameter is modified at random, and if the resulting model is better than the previously best-known model, the change is kept. Otherwise, it is reverted, and another tweak is tried. The given implementation assumes that the hill climbing algorithm is run after doing random search, so there is a promising model to start from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for hill climbing and evolutionary algorithm.\n",
    "def tweak_model(model_definition):\n",
    "    layer_num = np.random.randint(0, len(model_definition))\n",
    "    last_layer = len(model_definition) - 1\n",
    "    for first_dense, layer in enumerate(model_definition):\n",
    "        if layer['layer_type'] == 'DENSE':\n",
    "            break\n",
    "    if np.random.randint(0, 2) == 1:\n",
    "        delta = 1\n",
    "    else:\n",
    "        delta = -1\n",
    "    if np.random.randint(0, 2) == 1:\n",
    "        # Add/remove layer.\n",
    "        if len(model_definition) < 3:\n",
    "            delta = 1 # Layer removal not allowed\n",
    "        if delta == -1:\n",
    "            # Remove layer.\n",
    "            if layer_num == 0 and first_dense == 1:\n",
    "                layer_num += 1 # Require >= 1 non-dense layer\n",
    "            if layer_num == first_dense and layer_num == last_layer:\n",
    "                layer_num -= 1 # Require >= 1 dense layer\n",
    "            del model_definition[layer_num]\n",
    "        else:\n",
    "            # Add layer.\n",
    "            if layer_num < first_dense:\n",
    "                layer_type = layer_types[np.random.randint(1, 3)]\n",
    "            else:\n",
    "                layer_type = 'DENSE'\n",
    "            layer = generate_random_layer(layer_type)\n",
    "            model_definition.insert(layer_num, layer)\n",
    "    else:\n",
    "        # Tweak parameter.\n",
    "        layer = model_definition[layer_num]\n",
    "        layer_type = layer['layer_type']\n",
    "        params = layer_params[layer_type]\n",
    "        param = params[np.random.randint(0, len(params))]\n",
    "        current_val = layer[param]\n",
    "        values = param_values[param]\n",
    "        index = values.index(current_val)\n",
    "        max_index = len(values)\n",
    "        new_val = values[(index + delta) % max_index]\n",
    "        layer[param] = new_val\n",
    "\n",
    "# Hill climbing, starting from best model from random search.\n",
    "model_definition = best_model\n",
    "\n",
    "for i in range(CANDIDATE_EVALUATIONS):\n",
    "    valid_model = False\n",
    "    while(valid_model == False):\n",
    "        old_model_definition = copy.deepcopy(model_definition)\n",
    "        tweak_model(model_definition)\n",
    "        acc = create_and_evaluate_model(model_definition)\n",
    "        if acc > 0.0:\n",
    "            valid_model = True\n",
    "        else:\n",
    "            model_definition = old_model_definition\n",
    "    if acc > val_accuracy:\n",
    "        best_model = copy.deepcopy(model_definition)\n",
    "        val_accuracy = acc\n",
    "    else:\n",
    "        model_definition = old_model_definition\n",
    "    print('Hill climbing, best accuracy: %5.2f' %val_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both the random search algorithm and the hill climbing algorithm, our evaluation strategy was to evaluate each solution for only three epochs. We made the assumption that the resulting validation error would be a good indicator of how well the model would perform after more training. To get a more accurate evaluation of how well the best model actually performs, our final code snippet evaluates the best model for 20 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final model for larger number of epochs.\n",
    "model = create_model(best_model)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    train_images, train_labels, validation_data =\n",
    "    (test_images, test_labels), epochs=FINAL_EPOCHS, batch_size=64,\n",
    "    verbose=2, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
