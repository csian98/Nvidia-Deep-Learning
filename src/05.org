#+TITLE: 05.org
#+AUTHOR: Jeong Hoon Choi
#+DATE: 2024.02.04

* DL Framework
#+begin_src python :results output

import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.utils import to_categorical
import numpy as np
import logging

os.environ["TF_CPP_MIN_LOG_LEVEL"]="2"
tf.get_logger().setLevel(logging.ERROR)
tf.random.set_seed(2024)

EPOCHS=20
BATCH_SIZE=1

mnist=keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels)=mnist.load_data()

mean=np.mean(train_images)
stddev=np.std(train_images)
train_images=(train_images-mean)/stddev
test_images=(test_images-mean)/stddev

train_labels=to_categorical(train_labels, num_classes=10)
test_labels=to_categorical(test_labels, num_classes=10)

initializer=keras.initializers.RandomUniform(
    minval=-0.1, maxval=0.1
)

model=keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(25,
                       activation="tanh",
                       kernel_initializer=initializer,
                       bias_initializer="zeros"),
    keras.layers.Dense(10,
                       activation="sigmoid",
                       kernel_initializer=initializer,
                       bias_initializer="zeros"
                       )
])

opt=keras.optimizers.SGD(learning_rate=0.01)
model.compile(loss="mean_squared_error",
              optimizer=opt,
              metircs=["accuracy"]
              )
history=model.fit(train_images,
                  train_labels,
                  validation_data=(test_images, test_labels),
                  epochs=EPOCHS, batch_size=BATCH_SIZE,
                  verbose=2, shuffle=True
                  )

#+end_src

#+RESULTS:

** Initializers
#+begin_src python

initializer=keras.initializers.glorot_uniform()
initializer=keras.initializers.he_normal()

model=keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(25, activation="tanh",
                       kernel_initializer="glorot_uniform",
                       bias_initializer="zeros")
    keras.layers.Dense(10, activation="sigmoid",
                       kernel_initializer="glorot_uniform",
                       bias_initializer="zeros")
])

#+end_src

** Cross-Entropy
#+begin_src python

def cross_entropy(y_truth, y_predict):
    if y_truth==1.0:
        return -np.log(y_predict)
    else:
        return -np.log(1.0-y_predict)

model.compile(loss="binary_crossentropy", optimizer=optimizer_type, metrics=["accuracy"])
    
#+end_src

** Optimizers
#+begin_src python

opt=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)
opt=keras.optimizers.Adagrad(lr=0.01, epsilon=None)
opt=keras.optimizers.RMSprop(lr=0.001, rho=0.8, epsilon=None)
opt=keras.optimizers.Adam(lr=0.01, epsilon=0.1, decay=0.0)

#+end_src

* Hyperparameter tuning
#+begin_src python

model=keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(25, activation="relu",
                       kernal_initializer="he_normal",
                       bias_initializer="zeros"),
    keras.layers.Dense(10, activation="softmax",
                       kernal_initializer="glorot_uniform",
                       bias_initializer="zeros")
])

model.compile(loss="categorical_crossentropy",
              optimizer="adam",
              metrics=["accuracy"])

history=model.fit(train_images, train_labels,
                  validation_data=(test_images, test_labels),
                  epochs=EPOCHS, batch_size=BATCH_SIZE,
                  verbose=2, shuffle=True)

#+end_src
